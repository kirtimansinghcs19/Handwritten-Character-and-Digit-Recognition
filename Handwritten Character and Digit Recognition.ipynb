{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2b57d7",
   "metadata": {},
   "source": [
    "# Import tensorflow and dataset\n",
    "Using a Convolutional Neural Network that better suited for image processing. CNN's are less sensitive to where in the image the pattern we're looking for with multi-layer percetron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7885fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a73eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7ff16",
   "metadata": {},
   "source": [
    "Load up our raw data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a67baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7acfd076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images /= 255\n",
    "test_images /= 255\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa5e4a",
   "metadata": {},
   "source": [
    "As before we need to convert our train and test labels to be categorical in one_hot format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39bcb802",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels =tensorflow.keras.utils.to_categorical(mnist_train_labels, 10)\n",
    "test_labels =tensorflow.keras.utils.to_categorical(mnist_test_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d91ac6",
   "metadata": {},
   "source": [
    "# Training\n",
    "print out one of the training images with its label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edba39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d69cc401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_sample(num):\n",
    "    #Print the one-hot array of this sample's label\n",
    "    print(train_labels[num])\n",
    "    #Print the labels converted back to a number\n",
    "    label = train_labels[num].argmax(axis=0)\n",
    "    #Reshape the 786 values to a 28x28 image\n",
    "    image = train_images[num].reshape([28,28])\n",
    "    plt.title('Sample: %d Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "display_sample(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888717b",
   "metadata": {},
   "source": [
    "Now for the meat of the problem, Setting up a CNN involves more layers. Not all of these are strictly necessary; we could run without pooling and dropout, but those extra steps help avoid overfitting and helps things run faster.\n",
    "\n",
    "we'll start with a 2D convolution of the image- it's set up to take 32 windows, or filters, of each image, each filter being 3x3 in size.\n",
    "\n",
    "We run a second convolution on top of that with 64 3x3 windows- this topology is just what comes recommended within keras's own example. Again we want to re-use previous research whenever possible while tuning CNN, as is hard to do.\n",
    "\n",
    "Next we Apply MaxPooling2D layer that takes the maximum of each 2x2 result to distill the results down into something more manageable.\n",
    "\n",
    "A dropout filter is then applied to prevent overfitting.\n",
    "\n",
    "Next we Flatten the 2D layer we have at this stage into 1D layer.\n",
    "\n",
    "Then applying dropout again to further prevent overfitting\n",
    "\n",
    "And finally, we feed into our final 10 units where softmax is applied to choose our category of 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b6573a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size =(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "# 64  3x3 kernels\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# Reduce by taking the maxof each 2x2 block\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Dropout to avoid overfitting\n",
    "model.add(Dropout(0.25))\n",
    "# Flatten the results to one dimension for passing into our final layer\n",
    "model.add(Flatten())\n",
    "# A hidden layer to learn with\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Another dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Final categorization from 0-9 with software\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d98a2",
   "metadata": {},
   "source": [
    "Model Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af466339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae535f",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21086480",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6fd44c",
   "metadata": {},
   "source": [
    "This takes long time to run, print progress as each epoch is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d904fe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 65s - loss: 0.1869 - accuracy: 0.9433 - val_loss: 0.0444 - val_accuracy: 0.9850\n",
      "Epoch 2/10\n",
      "1875/1875 - 76s - loss: 0.0789 - accuracy: 0.9767 - val_loss: 0.0423 - val_accuracy: 0.9866\n",
      "Epoch 3/10\n",
      "1875/1875 - 79s - loss: 0.0622 - accuracy: 0.9814 - val_loss: 0.0311 - val_accuracy: 0.9904\n",
      "Epoch 4/10\n",
      "1875/1875 - 78s - loss: 0.0509 - accuracy: 0.9845 - val_loss: 0.0294 - val_accuracy: 0.9907\n",
      "Epoch 5/10\n",
      "1875/1875 - 78s - loss: 0.0435 - accuracy: 0.9865 - val_loss: 0.0298 - val_accuracy: 0.9911\n",
      "Epoch 6/10\n",
      "1875/1875 - 78s - loss: 0.0376 - accuracy: 0.9877 - val_loss: 0.0261 - val_accuracy: 0.9918\n",
      "Epoch 7/10\n",
      "1875/1875 - 79s - loss: 0.0312 - accuracy: 0.9902 - val_loss: 0.0260 - val_accuracy: 0.9916\n",
      "Epoch 8/10\n",
      "1875/1875 - 78s - loss: 0.0280 - accuracy: 0.9908 - val_loss: 0.0241 - val_accuracy: 0.9930\n",
      "Epoch 9/10\n",
      "1875/1875 - 79s - loss: 0.0256 - accuracy: 0.9920 - val_loss: 0.0285 - val_accuracy: 0.9924\n",
      "Epoch 10/10\n",
      "1875/1875 - 79s - loss: 0.0237 - accuracy: 0.9921 - val_loss: 0.0313 - val_accuracy: 0.9920\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                   batch_size=32,\n",
    "                   epochs=10,\n",
    "                   verbose=2,\n",
    "                   validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612bd5dc",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3e8b3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.031333744525909424\n",
      "Test accuracy: 0.9919999837875366\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:',score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fd56c",
   "metadata": {},
   "source": [
    "It has good accuracy of 99% but in real world it is not worth. If we building something where life and death are on the line like a self-driving car every fraction of percent matters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
